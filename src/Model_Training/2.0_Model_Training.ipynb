{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2495f1a",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "823c3aa2",
   "metadata": {},
   "source": [
    "### Data feed pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e7fa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:47:58.066881: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-13 12:48:00.066247: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/anuradha/anaconda3/envs/tfcpu/lib/python3.9/site-packages/cv2/../../../../lib:\n",
      "2023-01-13 12:48:00.066262: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-13 12:48:10.543676: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/anuradha/anaconda3/envs/tfcpu/lib/python3.9/site-packages/cv2/../../../../lib:\n",
      "2023-01-13 12:48:10.543740: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/anuradha/anaconda3/envs/tfcpu/lib/python3.9/site-packages/cv2/../../../../lib:\n",
      "2023-01-13 12:48:10.543746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "print(f'Tensorflow version:{tf.__version__}')\n",
    "\n",
    "import tqdm\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython import display\n",
    "from tensorflow_docs.vis import embed  #pip install -q git+https://github.com/tensorflow/docs\n",
    "\n",
    "maxPowerLimit = 25 # Maximum power limit of the solar panel. If power exceeds this level it will be capped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79334738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "# print(tf.test.is_built_with_cuda())\n",
    "# tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "251a0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "  \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "    \n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "  \"\"\"\n",
    "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "  return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46db9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
    "  \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "  \"\"\"\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  src = cv.VideoCapture(str(video_path))  \n",
    "\n",
    "  video_length = src.get(cv.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  src.set(cv.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame, output_size))\n",
    "\n",
    "  for _ in range(n_frames - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "    if ret:\n",
    "      frame = format_frames(frame, output_size)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12eed5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video_path = '/media/anuradha/Volume_A/Anuradha/Projects/Solar_Prediction/Src/Testing/Train/4/4_0465.avi'\n",
    "sample_video = frames_from_video_file(sample_video_path, n_frames = 10)\n",
    "sample_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03cce57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gif(images):\n",
    "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
    "  imageio.mimsave('./animation.gif', converted_images, fps=10)\n",
    "  return embed.embed_file('./animation.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4eae4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gif(sample_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59a0239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameGenerator:\n",
    "  def __init__(self, path, n_frames, training = False):\n",
    "    \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "      Args:\n",
    "        path: Video file paths.\n",
    "        n_frames: Number of frames. \n",
    "        training: Boolean to determine if training dataset is being created.\n",
    "    \"\"\"\n",
    "    self.path = path\n",
    "    self.n_frames = n_frames\n",
    "    self.training = training\n",
    "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "  def get_files_and_class_names(self):\n",
    "    video_paths = list(self.path.glob('*/*.avi'))\n",
    "    classes = [p.parent.name for p in video_paths] \n",
    "    return video_paths, classes\n",
    "\n",
    "  def __call__(self):\n",
    "    video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "    pairs = list(zip(video_paths, classes))\n",
    "\n",
    "    if self.training:\n",
    "      random.shuffle(pairs)\n",
    "\n",
    "    for path, name in pairs:\n",
    "      video_frames = frames_from_video_file(path, self.n_frames) \n",
    "      label = self.class_ids_for_name[name] # Encode labels\n",
    "      yield video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4374214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:48:21.001431: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10, 224, 224, 3)\n",
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "train_path = pathlib.Path('./SplitDataset/train')\n",
    "test_path = pathlib.Path('./SplitDataset/test')\n",
    "val_path = pathlib.Path('./SplitDataset/validate')\n",
    "\n",
    "fg = FrameGenerator(train_path, 10, training=True)\n",
    "\n",
    "frames, label = next(fg())\n",
    "\n",
    "print(f\"Shape: {frames.shape}\")\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d58e9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set\n",
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32), tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(train_path, 10, training=True),output_signature = output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baa9b484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int16)\n",
      "tf.Tensor(2, shape=(), dtype=int16)\n",
      "tf.Tensor(1, shape=(), dtype=int16)\n",
      "tf.Tensor(0, shape=(), dtype=int16)\n",
      "tf.Tensor(2, shape=(), dtype=int16)\n",
      "tf.Tensor(2, shape=(), dtype=int16)\n",
      "tf.Tensor(0, shape=(), dtype=int16)\n",
      "tf.Tensor(0, shape=(), dtype=int16)\n",
      "tf.Tensor(0, shape=(), dtype=int16)\n",
      "tf.Tensor(1, shape=(), dtype=int16)\n"
     ]
    }
   ],
   "source": [
    "for frames, labels in train_ds.take(10):\n",
    "  print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df17dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation set\n",
    "val_ds = tf.data.Dataset.from_generator(FrameGenerator(val_path, 10),output_signature = output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6efcb9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set of frames: (10, 224, 224, 3)\n",
      "Shape of training labels: ()\n",
      "Shape of validation set of frames: (10, 224, 224, 3)\n",
      "Shape of validation labels: ()\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the data\n",
    "train_frames, train_labels = next(iter(train_ds))\n",
    "print(f'Shape of training set of frames: {train_frames.shape}')\n",
    "print(f'Shape of training labels: {train_labels.shape}')\n",
    "\n",
    "val_frames, val_labels = next(iter(val_ds))\n",
    "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
    "print(f'Shape of validation labels: {val_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6663190",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eb80a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:48:33.208806: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 513 of 1000\n",
      "2023-01-13 12:48:42.658936: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n",
      "2023-01-13 12:48:42.719989: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set of frames: (2, 10, 224, 224, 3)\n",
      "Shape of training labels: (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:48:52.848841: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 494 of 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set of frames: (2, 10, 224, 224, 3)\n",
      "Shape of validation labels: (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:49:02.807685: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n",
      "2023-01-13 12:49:02.849292: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_ds.batch(2)\n",
    "val_ds = val_ds.batch(2)\n",
    "\n",
    "train_frames, train_labels = next(iter(train_ds))\n",
    "print(f'Shape of training set of frames: {train_frames.shape}')\n",
    "print(f'Shape of training labels: {train_labels.shape}')\n",
    "\n",
    "val_frames, val_labels = next(iter(val_ds))\n",
    "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
    "print(f'Shape of validation labels: {val_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0c86808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    131/Unknown - 40s 257ms/step - loss: 1.1900 - accuracy: 0.3321"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:50:50.029708: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 499 of 1000\n",
      "2023-01-13 12:51:00.028303: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 994 of 1000\n",
      "2023-01-13 12:51:00.147469: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 338s 3s/step - loss: 1.1900 - accuracy: 0.3321 - val_loss: 1.0953 - val_accuracy: 0.3820\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 301s 2s/step - loss: 1.0955 - accuracy: 0.3893 - val_loss: 1.0990 - val_accuracy: 0.3333\n",
      "Epoch 3/10\n",
      "131/131 [==============================] - 302s 2s/step - loss: 1.0245 - accuracy: 0.4618 - val_loss: 0.9861 - val_accuracy: 0.5535\n",
      "Epoch 4/10\n",
      "131/131 [==============================] - 284s 2s/step - loss: 0.9988 - accuracy: 0.5344 - val_loss: 1.1748 - val_accuracy: 0.3424\n",
      "Epoch 5/10\n",
      "131/131 [==============================] - 283s 2s/step - loss: 0.9771 - accuracy: 0.5420 - val_loss: 1.0151 - val_accuracy: 0.3541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2227f7b6a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.applications.EfficientNetB0(include_top = False)\n",
    "net.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(scale=255),\n",
    "    tf.keras.layers.TimeDistributed(net),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.GlobalAveragePooling3D()\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds, \n",
    "          epochs = 10,\n",
    "          validation_data = val_ds,\n",
    "          callbacks = tf.keras.callbacks.EarlyStopping(patience = 2, monitor = 'val_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4690f4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75128b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfcpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d06c60ee0440f4cc4d96a6818982dabc5793239f783ff296566ccfed4611f72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
